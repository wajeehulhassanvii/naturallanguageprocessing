{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUANTIFY MOVIE REVIEW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark-nlp\n",
      "  Downloading spark_nlp-2.5.1-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-2.5.1\n"
     ]
    }
   ],
   "source": [
    "! pip install  spark-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = spark.sparkContext.wholeTextFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**always negociate with stakeholders** As you work on the project, this list may change. The earlier you catch missed constraints the better. If you discover a constraint just before deployment, it can be very expensive to fix. This is why we want to iterate with stakeholders during development.  Now that we have listed our constraints, let’s discuss how we can build our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbEr.txt  imdb.vocab\tREADME\ttest  train\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../datasets/sparkNlp_imdb_review/aclImdb_v1/aclImdb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = spark.sparkContext.wholeTextFiles('../datasets/sparkNlp_imdb_review/aclImdb_v1/aclImdb/train/pos/')\n",
    "neg_train = spark.sparkContext.wholeTextFiles('../datasets/sparkNlp_imdb_review/aclImdb_v1/aclImdb/train/neg/')\n",
    "pos_test = spark.sparkContext.wholeTextFiles('../datasets/sparkNlp_imdb_review/aclImdb_v1/aclImdb/test/pos/')\n",
    "neg_test = spark.sparkContext.wholeTextFiles('../datasets/sparkNlp_imdb_review/aclImdb_v1/aclImdb/test/neg/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = spark.createDataFrame(pos_train, ['path', 'text'])\n",
    "pos_train = pos_train.repartition(100)\n",
    "pos_train = pos_train.withColumn('label', lit(1)).persist()\n",
    "\n",
    "neg_train = spark.createDataFrame(neg_train, ['path', 'text'])\n",
    "neg_train = neg_train.repartition(100)\n",
    "neg_train = neg_train.withColumn('label', lit(0)).persist()\n",
    "\n",
    "pos_test = spark.createDataFrame(pos_test, ['path', 'text'])\n",
    "pos_test = pos_test.repartition(100)\n",
    "pos_test = pos_test.withColumn('label', lit(1)).persist()\n",
    "\n",
    "neg_test = spark.createDataFrame(neg_test, ['path', 'text'])\n",
    "neg_test = neg_test.repartition(100)\n",
    "neg_test = neg_test.withColumn('label', lit(0)).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most heart-warming foreign films I've ever seen.<br /><br />The young girl is an amazing talent. Stellar performances by her (Doggie), the old man (the king of masks), and Liang (the Living Boddhisatva).<br /><br />(SPOILER) The deplorable treatment of children, especially females is disturbing.<br /><br />Loved the music. The original Chinese dialog heightens the emotional intensity of the performances and the story.<br /><br />This is a MUST SEE -- enjoyable family film, although not for very young children. Would have rated the DVD release even higher if the soundtrack had been transferred better onto the DVD and the transfer had included the widescreen version.\n"
     ]
    }
   ],
   "source": [
    "print(pos_train.first()['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is the worst movie i have ever seen... it is humorous how bad it is.. the entire time i was watching it i half expected music to start and the doctor starts dancing..(i've seen porno's with a better plot) When the raptor was trying to get in the door i think someone was throwing a plastic doll against the door from about 2 feet away. But as i said it is so bad you need to watch it so that you can see just how bad it is me explaining it isn't going to do anything compared to if you watch it .. i don't recommend renting it but if it comes on TV watch it for about 30min just to see what i mean. I couldn't watch more than 30min but if you can sit through the whole thing then you have some good willpower\n"
     ]
    }
   ],
   "source": [
    "print(neg_train.first()['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check the corpus as a whole "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_train_size: 12500\n",
      "neg_train_size: 12500\n",
      "pos_test_size: 12500\n",
      "neg_test_size: 12500\n"
     ]
    }
   ],
   "source": [
    "print('pos_train_size:', pos_train.count())\n",
    "print('neg_train_size:', neg_train.count())\n",
    "print('pos_test_size:', pos_test.count())\n",
    "print('neg_test_size:', neg_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check stats about the length of text in pos_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1347.160240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1046.747365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>695.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>982.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13704.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text_len\n",
       "count  12500.000000\n",
       "mean    1347.160240\n",
       "std     1046.747365\n",
       "min       70.000000\n",
       "25%      695.000000\n",
       "50%      982.000000\n",
       "75%     1651.000000\n",
       "max    13704.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_train.selectExpr('length(text) AS text_len').toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our project is divided into two parts\n",
    "1. training and measuring the model\n",
    "2. building the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### combing positive and negative into two datasets, train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pos_train.unionAll(neg_train)\n",
    "test = pos_test.unionAll(neg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s use Spark NLP to process the data. We will save both the lemmatized and normalized tokens, as well as GloVe embeddings. This way, we can experiment with different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEATING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "assembler = DocumentAssembler()\\\n",
    "            .setInputCol('text')\\\n",
    "            .setOutputCol('document')\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "            .setInputCols(['document'])\\\n",
    "            .setOutputCol('sentences')\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "            .setInputCols(['sentences'])\\\n",
    "            .setOutputCol('tokens')\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained()\\\n",
    "            .setInputCols(['tokens'])\\\n",
    "            .setOutputCol('lemmas')\n",
    "\n",
    "normalizer = Normalizer()\\\n",
    "            .setCleanupPatterns([\n",
    "            '[^a-zA-Z.-]+',\n",
    "            '^[^a-zA-Z]+',\n",
    "            '[^a-zA-Z]+$'\n",
    "            ])\\\n",
    "            .setInputCols(['lemmas'])\\\n",
    "            .setOutputCol('normalized')\\\n",
    "            .setLowercase(True)\n",
    "\n",
    "glove = WordEmbeddingsModel.pretrained(name='glove_100d')\\\n",
    "            .setInputCols(['document', 'normalized'])\\\n",
    "            .setOutputCol('embeddings')\n",
    "\n",
    "nlp_pipeline = Pipeline().setStages([\n",
    "    assembler, sentence, tokenizer, lemmatizer, normalizer, glove\n",
    "]).fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s select just the values we are interested in, namely the original data plus the normalized tokens and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = nlp_pipeline.transform(train)\\\n",
    "        .selectExpr(\n",
    "    'path', 'text', 'label',\n",
    "    'normalized.result AS normalized',\n",
    "    'embeddings.embeddings'\n",
    ")\n",
    "\n",
    "test = nlp_pipeline.transform(test)\\\n",
    "    .selectExpr('path', 'text', 'label',\n",
    "                'normalized.result AS normalized',\n",
    "                'embeddings.embeddings'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints  datasets  notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pipeline.write().overwrite().save('../checkpoints/nlp_pipeline.3.12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  we use simplest version of word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, text: string, label: int, normalized: array<string>, avg_wordvec: vector]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "\n",
    "def avg_wordvecs_fun(wordvecs):\n",
    "    return DenseVector(np.mean(wordvecs, axis=0))\n",
    "\n",
    "avg_wordvecs = spark.udf.register(\n",
    "    'avg_wordvecs',\n",
    "    avg_wordvecs_fun,\n",
    "    returnType=VectorUDT()\n",
    "    )\n",
    "\n",
    "train = train.withColumn('avg_wordvec', avg_wordvecs('embeddings'))\n",
    "test = test.withColumn('avg_wordvec', avg_wordvecs('embeddings'))\n",
    "\n",
    "train.drop('embeddings')\n",
    "test.drop('embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to save some space we will save it as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.write.mode('overwrite').parquet('../checkpoints/nlp_pipeline.3.12imdb.train')\n",
    "test.write.mode('overwrite').parquet('../checkpoints/nlp_pipeline.3.12imdb.test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  unpersist to save clean up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, text: string, label: int]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_train.unpersist()\n",
    "pos_test.unpersist()\n",
    "neg_train.unpersist()\n",
    "neg_test.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now load data from parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.parquet('../checkpoints/nlp_pipeline.3.12imdb.train/').persist()\n",
    "test = spark.read.parquet('../checkpoints/nlp_pipeline.3.12imdb.test/').persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try model with simple TF.IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = CountVectorizer()\\\n",
    "    .setInputCol('normalized')\\\n",
    "    .setOutputCol('tf')\n",
    "\n",
    "idf = IDF()\\\n",
    "    .setInputCol('tf')\\\n",
    "    .setOutputCol('tfidf')\n",
    "\n",
    "featurizer = Pipeline().setStages([tf, idf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have our feature, building model with **logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler()\\\n",
    "        .setInputCols(['avg_wordvec'])\\\n",
    "        .setOutputCol('features')\n",
    "\n",
    "logreg = LogisticRegression()\\\n",
    "        .setFeaturesCol('features')\\\n",
    "        .setLabelCol('label')\n",
    "\n",
    "model_pipeline = Pipeline()\\\n",
    "        .setStages([featurizer, vec_assembler, logreg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now saving the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save('../checkpoints/model.3.12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we will fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATING THE LINEAR REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator()\\\n",
    "    .setMetricName('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8027996352582054"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8012334966780377"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEPLOYMENT OF THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this app we only write a script. Otherwise offline \"deployment\" often involve creating a workflow which can be run preiodically. For this app we will create a script that can be run for new reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile movie_review_analysis.py\n",
    "\n",
    "\"\"\"\n",
    "This script will take file containing reviews of the same.\n",
    "It will output the results of analysis to std.out.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
